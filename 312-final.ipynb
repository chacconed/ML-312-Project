{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T13:41:52.587661Z","iopub.execute_input":"2021-07-28T13:41:52.588049Z","iopub.status.idle":"2021-07-28T13:41:59.841902Z","shell.execute_reply.started":"2021-07-28T13:41:52.587956Z","shell.execute_reply":"2021-07-28T13:41:59.841013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nRavdess_song = \"/kaggle/input/ravdess-emotional-song-audio/audio_song_actors_01-24/\"\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:41:59.843224Z","iopub.execute_input":"2021-07-28T13:41:59.843505Z","iopub.status.idle":"2021-07-28T13:41:59.84921Z","shell.execute_reply.started":"2021-07-28T13:41:59.843471Z","shell.execute_reply":"2021-07-28T13:41:59.848546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are different directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:02.532534Z","iopub.execute_input":"2021-07-28T13:42:02.533025Z","iopub.status.idle":"2021-07-28T13:42:03.123868Z","shell.execute_reply.started":"2021-07-28T13:42:02.53298Z","shell.execute_reply":"2021-07-28T13:42:03.122858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list_song = os.listdir(Ravdess_song)\nfile_emotion_song = []\nfile_path_song = []\nfor dir in ravdess_directory_list_song:\n    # as their are different directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess_song + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion_song.append(int(part[2]))\n        file_path_song.append(Ravdess_song + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df_song = pd.DataFrame(file_emotion_song, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df_song = pd.DataFrame(file_path_song, columns=['Path'])\nRavdess_df_song = pd.concat([emotion_df_song, path_df_song], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df_song.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df_song.describe()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:03.319319Z","iopub.execute_input":"2021-07-28T13:42:03.319669Z","iopub.status.idle":"2021-07-28T13:42:03.505797Z","shell.execute_reply.started":"2021-07-28T13:42:03.319632Z","shell.execute_reply":"2021-07-28T13:42:03.504681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ravdess_df = pd.concat([Ravdess_df,Ravdess_df_song])\n\nRavdess_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:04.649407Z","iopub.execute_input":"2021-07-28T13:42:04.649747Z","iopub.status.idle":"2021-07-28T13:42:04.669242Z","shell.execute_reply.started":"2021-07-28T13:42:04.649715Z","shell.execute_reply":"2021-07-28T13:42:04.66861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = Ravdess_df\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:07.146475Z","iopub.execute_input":"2021-07-28T13:42:07.146958Z","iopub.status.idle":"2021-07-28T13:42:07.171598Z","shell.execute_reply.started":"2021-07-28T13:42:07.146924Z","shell.execute_reply":"2021-07-28T13:42:07.170561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Count of Emotions', size=16)\nsns.countplot(data_path.Emotions)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T16:48:31.862647Z","iopub.execute_input":"2021-07-28T16:48:31.863411Z","iopub.status.idle":"2021-07-28T16:48:31.927599Z","shell.execute_reply.started":"2021-07-28T16:48:31.863305Z","shell.execute_reply":"2021-07-28T16:48:31.926587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for the data\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:11.349607Z","iopub.execute_input":"2021-07-28T13:42:11.349929Z","iopub.status.idle":"2021-07-28T13:42:12.217239Z","shell.execute_reply.started":"2021-07-28T13:42:11.3499Z","shell.execute_reply":"2021-07-28T13:42:12.216246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing a single audio file\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:12.845318Z","iopub.execute_input":"2021-07-28T13:42:12.845661Z","iopub.status.idle":"2021-07-28T13:42:13.033597Z","shell.execute_reply.started":"2021-07-28T13:42:12.845629Z","shell.execute_reply":"2021-07-28T13:42:13.032608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DATA AUGMENTATION\n\ndef noise(data):\n    noise_amp = 0.025*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.6):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\n\n# taking any example and checking for techniques.\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:16.517723Z","iopub.execute_input":"2021-07-28T13:42:16.518083Z","iopub.status.idle":"2021-07-28T13:42:16.693665Z","shell.execute_reply.started":"2021-07-28T13:42:16.518044Z","shell.execute_reply":"2021-07-28T13:42:16.692622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def extract_features(data):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally matlab equivalent result = [result zcr]\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n    \n    # tonnetz\n    tonn = np.mean(librosa.feature.tonnetz(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, tonn)) #stacking horizontally\n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    \n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) # stacking vertically\n    \n    # data with stretching and pitching\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = extract_features(data_stretch_pitch)\n    result = np.vstack((result, res3)) # stacking vertically\n    \n    return result # +1440*3 is our new number of data (number of rows) and columns are 6 features extracted.","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:21.561714Z","iopub.execute_input":"2021-07-28T13:42:21.562059Z","iopub.status.idle":"2021-07-28T13:42:21.572987Z","shell.execute_reply.started":"2021-07-28T13:42:21.562028Z","shell.execute_reply":"2021-07-28T13:42:21.571313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)\n\nzcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\nstft = np.abs(librosa.stft(data))\nchroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\nmfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\nrms = np.mean(librosa.feature.rms(y=data).T, axis=0)\nmel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n\nfeature = get_features(path)\n\nfeature.shape\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:25.255549Z","iopub.execute_input":"2021-07-28T13:42:25.256072Z","iopub.status.idle":"2021-07-28T13:42:27.852155Z","shell.execute_reply.started":"2021-07-28T13:42:25.256011Z","shell.execute_reply":"2021-07-28T13:42:27.850983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = np.array(data_path.Path)[1]\npath","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:28.904347Z","iopub.execute_input":"2021-07-28T13:42:28.904738Z","iopub.status.idle":"2021-07-28T13:42:28.910854Z","shell.execute_reply.started":"2021-07-28T13:42:28.904704Z","shell.execute_reply":"2021-07-28T13:42:28.909911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking the shapes of the features\nzcr.shape\nchroma_stft.shape\nmfcc.shape\nmfcc.shape\nmel.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:30.584543Z","iopub.execute_input":"2021-07-28T13:42:30.585168Z","iopub.status.idle":"2021-07-28T13:42:30.591713Z","shell.execute_reply.started":"2021-07-28T13:42:30.58512Z","shell.execute_reply":"2021-07-28T13:42:30.590701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\nfor path, emotion in zip(data_path.Path, data_path.Emotions):\n    feature = get_features(path) # outputs result matrix (1data*3) * (6)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T13:42:37.146615Z","iopub.execute_input":"2021-07-28T13:42:37.146949Z","iopub.status.idle":"2021-07-28T14:33:30.17944Z","shell.execute_reply.started":"2021-07-28T13:42:37.146919Z","shell.execute_reply":"2021-07-28T14:33:30.178051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y), data_path.Path.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:37:11.616354Z","iopub.execute_input":"2021-07-28T14:37:11.616905Z","iopub.status.idle":"2021-07-28T14:37:11.625935Z","shell.execute_reply.started":"2021-07-28T14:37:11.616851Z","shell.execute_reply":"2021-07-28T14:37:11.625181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)\nFeatures.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:37:13.175765Z","iopub.execute_input":"2021-07-28T14:37:13.176467Z","iopub.status.idle":"2021-07-28T14:37:15.785334Z","shell.execute_reply.started":"2021-07-28T14:37:13.1764Z","shell.execute_reply":"2021-07-28T14:37:15.784418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values\n\n# one hot encoding our y\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:37:17.538604Z","iopub.execute_input":"2021-07-28T14:37:17.538937Z","iopub.status.idle":"2021-07-28T14:37:17.555601Z","shell.execute_reply.started":"2021-07-28T14:37:17.538908Z","shell.execute_reply":"2021-07-28T14:37:17.554589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train test split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:44:38.844319Z","iopub.execute_input":"2021-07-28T14:44:38.844969Z","iopub.status.idle":"2021-07-28T14:44:38.862377Z","shell.execute_reply.started":"2021-07-28T14:44:38.844931Z","shell.execute_reply":"2021-07-28T14:44:38.861535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalizes the data by -miu/std.dev\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:44:45.958833Z","iopub.execute_input":"2021-07-28T14:44:45.959159Z","iopub.status.idle":"2021-07-28T14:44:45.984772Z","shell.execute_reply.started":"2021-07-28T14:44:45.959127Z","shell.execute_reply":"2021-07-28T14:44:45.983854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import savetxt\n\nsavetxt('xtest.csv', x_test, delimiter=',')\nsavetxt('xtrain.csv', x_train, delimiter = ',')\n\nsavetxt('ytest.csv', y_test, delimiter=',')\nsavetxt('ytrain.csv', y_train, delimiter = ',')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:45:25.439003Z","iopub.execute_input":"2021-07-28T14:45:25.439348Z","iopub.status.idle":"2021-07-28T14:45:26.838986Z","shell.execute_reply.started":"2021-07-28T14:45:25.439316Z","shell.execute_reply":"2021-07-28T14:45:26.838161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making our data compatible to model.\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:46:15.455724Z","iopub.execute_input":"2021-07-28T14:46:15.456078Z","iopub.status.idle":"2021-07-28T14:46:15.463084Z","shell.execute_reply.started":"2021-07-28T14:46:15.456047Z","shell.execute_reply":"2021-07-28T14:46:15.462064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the convolutional neural network that we are going to use\n\nmodel=Sequential()\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=8, activation='softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T14:46:19.992985Z","iopub.execute_input":"2021-07-28T14:46:19.993339Z","iopub.status.idle":"2021-07-28T14:46:20.113637Z","shell.execute_reply.started":"2021-07-28T14:46:19.993294Z","shell.execute_reply":"2021-07-28T14:46:20.11243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=3, min_lr== 0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=256, epochs=90, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:05:18.843787Z","iopub.execute_input":"2021-07-28T15:05:18.844165Z","iopub.status.idle":"2021-07-28T15:34:25.79385Z","shell.execute_reply.started":"2021-07-28T15:05:18.84413Z","shell.execute_reply":"2021-07-28T15:34:25.792801Z"},"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_weights(\"312weight.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:36:53.06822Z","iopub.execute_input":"2021-07-28T15:36:53.068652Z","iopub.status.idle":"2021-07-28T15:36:53.108607Z","shell.execute_reply.started":"2021-07-28T15:36:53.068616Z","shell.execute_reply":"2021-07-28T15:36:53.107244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('./')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:37:23.771183Z","iopub.execute_input":"2021-07-28T15:37:23.771664Z","iopub.status.idle":"2021-07-28T15:37:25.731252Z","shell.execute_reply.started":"2021-07-28T15:37:23.771602Z","shell.execute_reply":"2021-07-28T15:37:25.730258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:37:29.410009Z","iopub.execute_input":"2021-07-28T15:37:29.410377Z","iopub.status.idle":"2021-07-28T15:37:29.415537Z","shell.execute_reply.started":"2021-07-28T15:37:29.410339Z","shell.execute_reply":"2021-07-28T15:37:29.414852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:37:30.172259Z","iopub.execute_input":"2021-07-28T15:37:30.172857Z","iopub.status.idle":"2021-07-28T15:37:30.180719Z","shell.execute_reply.started":"2021-07-28T15:37:30.172814Z","shell.execute_reply":"2021-07-28T15:37:30.179054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy on validation : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:37:33.186837Z","iopub.execute_input":"2021-07-28T15:37:33.187198Z","iopub.status.idle":"2021-07-28T15:37:34.675039Z","shell.execute_reply.started":"2021-07-28T15:37:33.187165Z","shell.execute_reply":"2021-07-28T15:37:34.673799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visual aid of how the metrics vary with accuracy\n#plotting the losses\n\nepochs = [i for i in range(100)]\nfig , axis = plt.subplots(1,1)\n\n\ntrain_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\naxis.plot(epochs , train_loss , label = 'Training Loss')\naxis.plot(epochs , test_loss , label = 'Testing Loss')\naxis.set_title('Training & Testing Loss')\naxis.legend()\naxis.set_xlabel(\"Epochs\")\n\nplt.savefig('Losses.png')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:40:10.582746Z","iopub.execute_input":"2021-07-28T15:40:10.583107Z","iopub.status.idle":"2021-07-28T15:40:10.924131Z","shell.execute_reply.started":"2021-07-28T15:40:10.583076Z","shell.execute_reply":"2021-07-28T15:40:10.923001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the accuracy of the training and the validation set\nepochs = [i for i in range(100)]\nfig , axis = plt.subplots(1,1)\n\ntrain_accuracy = history.history['accuracy']\ntest_accuracy = history.history['val_accuracy']\n\nfig.set_size_inches(20,10)\naxis.plot(epochs , train_accuracy , label = 'Training Accuracy')\naxis.plot(epochs , test_accuracy , label = 'Testing Accuracy')\naxis.set_title('Training & Testing Accuracy')\naxis.legend()\naxis.set_xlabel(\"Epochs\")\nplt.show()\n\nplt.savefig('Accuracy_.png')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:41:12.846502Z","iopub.execute_input":"2021-07-28T15:41:12.847047Z","iopub.status.idle":"2021-07-28T15:41:13.088939Z","shell.execute_reply.started":"2021-07-28T15:41:12.84701Z","shell.execute_reply":"2021-07-28T15:41:13.0877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plotting the accuracy of the training and the validation set\nepochs = [i for i in range(100)]\nfig , axis = plt.subplots(1,1)\n\ntrain_accuracy = history.history['accuracy']\ntest_accuracy = history.history['val_accuracy']\n\nfig.set_size_inches(20,6)\naxis.plot(epochs , train_accuracy , label = 'Training Accuracy')\naxis.plot(epochs , test_accuracy , label = 'Testing Accuracy')\naxis.set_title('Training & Testing Accuracy')\naxis.legend()\naxis.set_xlabel(\"Epochs\")\nplt.show()\n\nplt.savefig('Accuracies.png')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:42:26.22621Z","iopub.execute_input":"2021-07-28T15:42:26.22658Z","iopub.status.idle":"2021-07-28T15:42:26.432005Z","shell.execute_reply.started":"2021-07-28T15:42:26.226548Z","shell.execute_reply":"2021-07-28T15:42:26.431057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Loads the weights #for pretrained weights see our directory\n    \nmodel.load_weights('../input/weightsss/312weight.h5')\n\n# Re-evaluate the model\nloss, acc = model.evaluate(x_test, y_test, verbose=2)\nprint(\"Restored model, accuracy: {:5.2f}%\".format(100 * acc))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:47:01.169131Z","iopub.execute_input":"2021-07-28T15:47:01.16959Z","iopub.status.idle":"2021-07-28T15:47:02.809418Z","shell.execute_reply.started":"2021-07-28T15:47:01.169549Z","shell.execute_reply":"2021-07-28T15:47:02.807819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}