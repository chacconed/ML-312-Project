{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-17T16:30:19.545916Z","iopub.execute_input":"2021-07-17T16:30:19.54624Z","iopub.status.idle":"2021-07-17T16:30:19.553147Z","shell.execute_reply.started":"2021-07-17T16:30:19.546213Z","shell.execute_reply":"2021-07-17T16:30:19.552126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Paths for data.\nRavdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n# Crema = \"/kaggle/input/cremad/AudioWAV/\"\nRavdess","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:20.662043Z","iopub.execute_input":"2021-07-17T16:30:20.662387Z","iopub.status.idle":"2021-07-17T16:30:20.669829Z","shell.execute_reply.started":"2021-07-17T16:30:20.662357Z","shell.execute_reply":"2021-07-17T16:30:20.66885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Ravdess)\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are different directory we need to extract files for each actor.\n    actor = os.listdir(Ravdess + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Ravdess + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()\n\n\nRavdess_df.describe()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:24.970526Z","iopub.execute_input":"2021-07-17T16:30:24.97083Z","iopub.status.idle":"2021-07-17T16:30:25.412197Z","shell.execute_reply.started":"2021-07-17T16:30:24.970803Z","shell.execute_reply":"2021-07-17T16:30:25.41129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = Ravdess_df\ndata_path.to_csv(\"data_path.csv\",index=False)\ndata_path.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:26.633117Z","iopub.execute_input":"2021-07-17T16:30:26.633715Z","iopub.status.idle":"2021-07-17T16:30:26.658585Z","shell.execute_reply.started":"2021-07-17T16:30:26.633683Z","shell.execute_reply":"2021-07-17T16:30:26.657925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Count of Emotions', size=16)\nsns.countplot(data_path.Emotions)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:29.376153Z","iopub.execute_input":"2021-07-17T16:30:29.376656Z","iopub.status.idle":"2021-07-17T16:30:29.566546Z","shell.execute_reply.started":"2021-07-17T16:30:29.376623Z","shell.execute_reply":"2021-07-17T16:30:29.56576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for the data\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:30.821881Z","iopub.execute_input":"2021-07-17T16:30:30.822174Z","iopub.status.idle":"2021-07-17T16:30:31.833634Z","shell.execute_reply.started":"2021-07-17T16:30:30.822147Z","shell.execute_reply":"2021-07-17T16:30:31.83282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:33.435056Z","iopub.execute_input":"2021-07-17T16:30:33.43556Z","iopub.status.idle":"2021-07-17T16:30:33.587781Z","shell.execute_reply.started":"2021-07-17T16:30:33.435528Z","shell.execute_reply":"2021-07-17T16:30:33.587202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DATA AUGMENTATION\n\ndef noise(data):\n    noise_amp = 0.025*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.6):\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n\n# taking any example and checking for techniques.\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:35.565789Z","iopub.execute_input":"2021-07-17T16:30:35.566192Z","iopub.status.idle":"2021-07-17T16:30:35.722628Z","shell.execute_reply.started":"2021-07-17T16:30:35.566165Z","shell.execute_reply":"2021-07-17T16:30:35.72186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def extract_features(data):\n    # ZCR\n    result = np.array([])\n    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n    result=np.hstack((result, zcr)) # stacking horizontally matlab equivalent result = [result zcr]\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, chroma_stft)) # stacking horizontally\n\n    # MFCC\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mfcc)) # stacking horizontally\n\n    # Root Mean Square Value\n    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n    result = np.hstack((result, rms)) # stacking horizontally\n\n    # MelSpectogram\n    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n    result = np.hstack((result, mel)) # stacking horizontally\n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data)\n    result = np.array(res1)\n    \n    \n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data)\n    result = np.vstack((result, res2)) # stacking vertically\n    \n    # data with stretching and pitching\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = extract_features(data_stretch_pitch)\n    result = np.vstack((result, res3)) # stacking vertically\n    \n    return result # 1440*3 is our new number of data (number of rows) and columns are 5 features extracted.","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:38.116912Z","iopub.execute_input":"2021-07-17T16:30:38.117208Z","iopub.status.idle":"2021-07-17T16:30:38.12774Z","shell.execute_reply.started":"2021-07-17T16:30:38.117182Z","shell.execute_reply":"2021-07-17T16:30:38.127172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)\n\nzcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\nstft = np.abs(librosa.stft(data))\nchroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\nmfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\nrms = np.mean(librosa.feature.rms(y=data).T, axis=0)\nmel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n\nfeature = get_features(path)\n\nfeature.shape\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:40.588866Z","iopub.execute_input":"2021-07-17T16:30:40.589315Z","iopub.status.idle":"2021-07-17T16:30:41.784728Z","shell.execute_reply.started":"2021-07-17T16:30:40.589285Z","shell.execute_reply":"2021-07-17T16:30:41.783495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = np.array(data_path.Path)[1]\npath","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:42.945852Z","iopub.execute_input":"2021-07-17T16:30:42.946144Z","iopub.status.idle":"2021-07-17T16:30:42.95088Z","shell.execute_reply.started":"2021-07-17T16:30:42.946119Z","shell.execute_reply":"2021-07-17T16:30:42.950133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zcr.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:44.014694Z","iopub.execute_input":"2021-07-17T16:30:44.015156Z","iopub.status.idle":"2021-07-17T16:30:44.020353Z","shell.execute_reply.started":"2021-07-17T16:30:44.015127Z","shell.execute_reply":"2021-07-17T16:30:44.019531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chroma_stft.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:44.856434Z","iopub.execute_input":"2021-07-17T16:30:44.856884Z","iopub.status.idle":"2021-07-17T16:30:44.861125Z","shell.execute_reply.started":"2021-07-17T16:30:44.856854Z","shell.execute_reply":"2021-07-17T16:30:44.860488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mfcc.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:45.845182Z","iopub.execute_input":"2021-07-17T16:30:45.845783Z","iopub.status.idle":"2021-07-17T16:30:45.851383Z","shell.execute_reply.started":"2021-07-17T16:30:45.845735Z","shell.execute_reply":"2021-07-17T16:30:45.850728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rms.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:46.568693Z","iopub.execute_input":"2021-07-17T16:30:46.569119Z","iopub.status.idle":"2021-07-17T16:30:46.573527Z","shell.execute_reply.started":"2021-07-17T16:30:46.569091Z","shell.execute_reply":"2021-07-17T16:30:46.572889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mel.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:49.12015Z","iopub.execute_input":"2021-07-17T16:30:49.12065Z","iopub.status.idle":"2021-07-17T16:30:49.125114Z","shell.execute_reply.started":"2021-07-17T16:30:49.120617Z","shell.execute_reply":"2021-07-17T16:30:49.124416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\ni = 0;\nfor path, emotion in zip(data_path.Path, data_path.Emotions):\n    print(i)\n    i = i+1\n    feature = get_features(path) # outputs result matrix (1data*3) * (5)\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:30:49.957105Z","iopub.execute_input":"2021-07-17T16:30:49.95753Z","iopub.status.idle":"2021-07-17T16:44:04.278127Z","shell.execute_reply.started":"2021-07-17T16:30:49.9575Z","shell.execute_reply":"2021-07-17T16:44:04.277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X), len(Y), data_path.Path.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:44:10.744309Z","iopub.execute_input":"2021-07-17T16:44:10.744698Z","iopub.status.idle":"2021-07-17T16:44:10.751087Z","shell.execute_reply.started":"2021-07-17T16:44:10.744665Z","shell.execute_reply":"2021-07-17T16:44:10.750226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:44:18.251807Z","iopub.execute_input":"2021-07-17T16:44:18.252156Z","iopub.status.idle":"2021-07-17T16:44:18.622437Z","shell.execute_reply.started":"2021-07-17T16:44:18.252129Z","shell.execute_reply":"2021-07-17T16:44:18.621581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)\nFeatures.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:44:21.789514Z","iopub.execute_input":"2021-07-17T16:44:21.790016Z","iopub.status.idle":"2021-07-17T16:44:23.72736Z","shell.execute_reply.started":"2021-07-17T16:44:21.789987Z","shell.execute_reply":"2021-07-17T16:44:23.726552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values\n\n# one hot encoding our y\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:44:26.540369Z","iopub.execute_input":"2021-07-17T16:44:26.540895Z","iopub.status.idle":"2021-07-17T16:44:26.552356Z","shell.execute_reply.started":"2021-07-17T16:44:26.540834Z","shell.execute_reply":"2021-07-17T16:44:26.551668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train test split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:44:28.323876Z","iopub.execute_input":"2021-07-17T16:44:28.324406Z","iopub.status.idle":"2021-07-17T16:44:28.335697Z","shell.execute_reply.started":"2021-07-17T16:44:28.324364Z","shell.execute_reply":"2021-07-17T16:44:28.335109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normalizes the data by -miu/std.dev\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:44:29.82084Z","iopub.execute_input":"2021-07-17T16:44:29.821434Z","iopub.status.idle":"2021-07-17T16:44:29.836198Z","shell.execute_reply.started":"2021-07-17T16:44:29.821386Z","shell.execute_reply":"2021-07-17T16:44:29.835496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making our data compatible to model.\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:45:24.181871Z","iopub.execute_input":"2021-07-17T16:45:24.182177Z","iopub.status.idle":"2021-07-17T16:45:24.188844Z","shell.execute_reply.started":"2021-07-17T16:45:24.18215Z","shell.execute_reply":"2021-07-17T16:45:24.187733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the convolutional neural network that we are going to use\n\nmodel=Sequential()\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(units=8, activation='softmax'))\nmodel.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T16:50:03.34836Z","iopub.execute_input":"2021-07-17T16:50:03.348712Z","iopub.status.idle":"2021-07-17T16:50:03.523208Z","shell.execute_reply.started":"2021-07-17T16:50:03.348686Z","shell.execute_reply":"2021-07-17T16:50:03.522339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=55, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2021-07-17T17:01:39.48777Z","iopub.execute_input":"2021-07-17T17:01:39.488369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}